{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAdrVPKAmboz"
   },
   "source": [
    "# Problem Set 1: Gaussian Mixture Models for Image Patches\n",
    "\n",
    "In this notebook, you will implement and analyze Gaussian Mixture Models (GMMs) for modeling natural image patches, and use them for image denoising.\n",
    "\n",
    "**Instructions:**\n",
    "- Look for `# TODO` comments indicating where you need to write code\n",
    "- Questions requiring written answers are in **Markdown cells** - double-click to edit\n",
    "- Each TODO corresponds to a specific sub-problem in the problem set\n",
    "- Run all cells in order; some later cells depend on earlier ones\n",
    "\n",
    "**References:**\n",
    "- Zoran & Weiss, \"Natural Images, Gaussian Mixtures and Dead Leaves\" (NIPS 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Br02lGmbo1"
   },
   "source": [
    "---\n",
    "## Google Colab & GPU Usage Best Practices\n",
    "\n",
    "*These instructions have been borrowed courtesy of https://www.cs.cornell.edu/courses/cs5670/2025sp*\n",
    "\n",
    "### Using Google Colab\n",
    "Google Colab is a cloud-based platform for running Python code, similar to Jupyter Notebooks. It requires no installation and saves your work directly to your Google Drive, just like Docs or Sheets.\n",
    "\n",
    "We recommend using a Colab notebook with **GPU enabled** to complete this project efficiently.\n",
    "\n",
    "**Note:** Colab has a free usage limit. If you're running long training sessionsâ€”especially close to the deadlineâ€”you may encounter resource restrictions. Some students work around this by:\n",
    "- Using a different Google account (within Google's usage policies)\n",
    "- Upgrading to Colab Pro (a paid option) for extended runtime and better resources\n",
    "\n",
    "### Colab Setup:\n",
    "1. Upload this `.ipynb` notebook template to your Google Drive\n",
    "2. Double clicking on the notebook on Google Drive should give you an option for opening it in Colab\n",
    "3. Alternatively, you can directly open Colab and upload notebook following this: `File -> Upload notebook...`\n",
    "4. If you haven't used Colab or Jupyter Notebooks before, first read the [Colaboratory welcome guide](https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "5. Colab requires almost no setup, so there is no need to install PyTorch locally\n",
    "\n",
    "### ðŸ’¡ Pro Tips for GPU Usage:\n",
    "- **Debug on CPU first!** Use CPU mode to test your code logic before switching to GPU. This avoids wasting GPU credits on buggy code.\n",
    "- To enable GPU: `Runtime -> Change runtime type -> Hardware accelerator -> GPU`\n",
    "- Only switch to GPU when you're ready for full training runs\n",
    "- If you run out of GPU quota, wait or try a different Google account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N30ks4PTmbo2"
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cglqis9qmbo2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3otmQx1mbo3"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the preprocessed BSDS500 patches. The data has been preprocessed to:\n",
    "- Extract 16x16 grayscale patches\n",
    "- Normalize to [0, 1]\n",
    "- Remove DC component (subtract mean from each patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whbjniOimmiw"
   },
   "outputs": [],
   "source": [
    "# Download the files that you'll need\n",
    "base_url = 'https://www.cs.cornell.edu/courses/cs5788/2026sp/data'\n",
    "for name in ['patches_train.npy', 'patches_test.npy', 'test_images.npy']:\n",
    "  with open(name, 'wb') as out:\n",
    "    url = os.path.join(base_url, name)\n",
    "    out.write(urllib.request.urlopen(url).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM9jC4Mjmbo3"
   },
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "patches_train = np.load('patches_train.npy')\n",
    "patches_test = np.load('patches_test.npy')\n",
    "test_images = np.load('test_images.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"Training patches: {patches_train.shape}\")\n",
    "print(f\"Test patches: {patches_test.shape}\")\n",
    "print(f\"Test images: {len(test_images)} images\")\n",
    "\n",
    "# Patch dimensions\n",
    "PATCH_DIM = 16\n",
    "PATCH_SIZE = PATCH_DIM * PATCH_DIM  # 256\n",
    "\n",
    "# Convert to PyTorch tensors for later use\n",
    "patches_train_tensor = torch.tensor(patches_train, dtype=torch.float32)\n",
    "patches_test_tensor = torch.tensor(patches_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIQMiIlxmbo3"
   },
   "source": [
    "### Visualize some training patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFToEBocmbo4"
   },
   "outputs": [],
   "source": [
    "def visualize_patches(patches, nrows=4, ncols=8, title=\"Patches\"):\n",
    "    \"\"\"Visualize a grid of patches.\"\"\"\n",
    "    n = nrows * ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 1.5, nrows * 1.5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n and i < len(patches):\n",
    "            patch = patches[i]\n",
    "            if len(patch.shape) == 1:\n",
    "                patch = patch.reshape(PATCH_DIM, PATCH_DIM)\n",
    "            ax.imshow(patch + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some training patches (add 0.5 since DC was removed)\n",
    "visualize_patches(patches_train[:32], title=\"Sample Training Patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHgNmH7cmbo4"
   },
   "source": [
    "---\n",
    "## Background: Maximum Likelihood Estimation for Gaussians\n",
    "\n",
    "### Single Gaussian MLE\n",
    "\n",
    "Given $N$ data points $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$ where each $\\mathbf{x}_i \\in \\mathbb{R}^D$, the Maximum Likelihood Estimates (MLE) for a single Gaussian distribution are:\n",
    "\n",
    "**Mean:**\n",
    "$$\\hat{\\boldsymbol{\\mu}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i$$\n",
    "\n",
    "**Covariance:**\n",
    "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^T$$\n",
    "\n",
    "These closed-form solutions minimize the negative log-likelihood:\n",
    "$$\\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\mathcal{N}(\\mathbf{x}_i | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fxByJm_mbo4"
   },
   "source": [
    "---\n",
    "## Helper Functions\n",
    "\n",
    "### GMM Negative Log-Likelihood\n",
    "\n",
    "We compute the negative log-likelihood of data under a Gaussian Mixture Model:\n",
    "$$\\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\right)$$\n",
    "\n",
    "The implementation uses:\n",
    "- Matrix operations for computing differences and Mahalanobis distances\n",
    "- Cholesky decomposition for numerical stability with covariance matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyyR1NwPmbo4"
   },
   "source": [
    "### Log-Determinant via Cholesky Decomposition\n",
    "\n",
    "**Why use Cholesky?** Computing $\\log|\\Sigma|$ directly can be slow and numerically unstable. Instead, we take advantage of special properties of the covariance matrix $\\Sigma$ (symmetric and positive definite) and use the Cholesky decomposition (which only works with symmetric and positive definite matrices).\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "For a positive definite matrix $\\Sigma$, the Cholesky decomposition gives us a **lower triangular** matrix $L$ such that:\n",
    "$$\\Sigma = LL^T$$\n",
    "\n",
    "Taking the determinant of both sides:\n",
    "$$|\\Sigma| = |LL^T| = |L| \\cdot |L^T| = |L|^2$$\n",
    "\n",
    "For a triangular matrix, the determinant is the product of diagonal elements:\n",
    "$$|L| = \\prod_{i=1}^{D} L_{ii}$$\n",
    "\n",
    "Therefore:\n",
    "$$|\\Sigma| = \\left(\\prod_{i=1}^{D} L_{ii}\\right)^2$$\n",
    "\n",
    "Taking the logarithm:\n",
    "$$\\log|\\Sigma| = 2 \\log\\left(\\prod_{i=1}^{D} L_{ii}\\right) = 2 \\sum_{i=1}^{D} \\log(L_{ii})$$\n",
    "\n",
    "In code: `log_det = 2 * sum(log(diag(L)))`\n",
    "\n",
    "This is numerically stable because we only need the diagonal elements of $L$, and we work in log-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ21oa54mbo4"
   },
   "source": [
    "### Background: Symmetric and Positive-Definite Matrices\n",
    "\n",
    "**Symmetric Matrix:** A matrix $A \\in \\mathbb{R}^{n \\times n}$ is **symmetric** if $A = A^T$, meaning $A_{ij} = A_{ji}$ for all $i, j$.\n",
    "\n",
    "**Positive-Definite Matrix:** A symmetric matrix $A$ is **positive-definite** if for all non-zero vectors $\\mathbf{x} \\in \\mathbb{R}^n$:\n",
    "$$\\mathbf{x}^T A \\mathbf{x} > 0$$\n",
    "\n",
    "A matrix is **positive semi-definite** if $\\mathbf{x}^T A \\mathbf{x} \\geq 0$ (allowing equality).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Covariance Matrices Are Symmetric and Positive Semi-Definite\n",
    "\n",
    "**Definition of Covariance Matrix:**\n",
    "$$\\Sigma = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]$$\n",
    "\n",
    "**Proof of Symmetry:**\n",
    "$$\\Sigma^T = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]^T = \\mathbb{E}[((\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T)^T] = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T] = \\Sigma$$\n",
    "\n",
    "**Proof of Positive Semi-Definiteness:**\n",
    "For any vector $\\mathbf{v} \\in \\mathbb{R}^n$:\n",
    "$$\\mathbf{v}^T \\Sigma \\mathbf{v} = \\mathbf{v}^T \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T] \\mathbf{v} = \\mathbb{E}[\\mathbf{v}^T(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T\\mathbf{v}]$$\n",
    "\n",
    "Let $Y = \\mathbf{v}^T(\\mathbf{X} - \\boldsymbol{\\mu})$, which is a scalar. Then:\n",
    "$$\\mathbf{v}^T \\Sigma \\mathbf{v} = \\mathbb{E}[Y^2] \\geq 0$$\n",
    "\n",
    "Since the expectation of a squared quantity is always non-negative, $\\Sigma$ is positive semi-definite.\n",
    "\n",
    "**Note:** In practice, we add a small regularization term $\\varepsilon I$ to ensure the covariance is strictly positive-definite (all eigenvalues $> 0$), which guarantees the Cholesky decomposition to exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcMls-s5mbo5"
   },
   "outputs": [],
   "source": [
    "def gmm_nll(mix, mu, cov, data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Compute GMM negative log-likelihood with FULL covariance.\n",
    "    Uses Cholesky decomposition for numerical stability.\n",
    "\n",
    "    Args:\n",
    "        mix: Mixture weights (K,)\n",
    "        mu: Means (K, D)\n",
    "        cov: Full covariance matrices (K, D, D)\n",
    "        data: Data points (N, D)\n",
    "\n",
    "    Returns:\n",
    "        Scalar NLL loss\n",
    "\n",
    "    HINT for computing Mahalanobis distance without einsum:\n",
    "    1. diff = x[:, None, :] - mu[None, :, :]  gives (N, K, D)\n",
    "    2. Use Cholesky: L = cholesky(cov), then solve L @ z = diff\n",
    "    3. Mahalanobis = ||z||^2 = sum(z^2)\n",
    "    \"\"\"\n",
    "\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    nll = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        x = data[i * batch_size : (i + 1) * batch_size]\n",
    "        N, D = x.shape\n",
    "        K = mix.shape[0]\n",
    "\n",
    "        # Compute difference: (N, K, D)\n",
    "        diff = x[:, None, :] - mu[None, :, :]\n",
    "\n",
    "        # Cholesky decomposition for numerical stability\n",
    "        L = torch.linalg.cholesky(cov)  # (K, D, D)\n",
    "\n",
    "        # Log determinant: log|cov| = 2 * sum(log(diag(L)))\n",
    "        log_det = 2 * torch.sum(torch.log(torch.diagonal(L, dim1=1, dim2=2)), dim=1)  # (K,)\n",
    "\n",
    "        # Mahalanobis distance using triangular solve\n",
    "        # Reshape for batch solve: diff is (N, K, D), need (K, N, D)\n",
    "        diff_flat = diff.permute(1, 0, 2)  # (K, N, D)\n",
    "\n",
    "        # Solve L @ z = diff^T for each component\n",
    "        solved = torch.linalg.solve_triangular(L, diff_flat.transpose(1, 2), upper=False)  # (K, D, N)\n",
    "\n",
    "        # Mahalanobis = sum of squared elements\n",
    "        mahal = torch.sum(solved ** 2, dim=1).T  # (N, K)\n",
    "\n",
    "        # Log probability\n",
    "        log_norm = -0.5 * (D * np.log(2 * np.pi) + log_det)  # (K,)\n",
    "        log_mix = torch.log(mix + 1e-12)  # (K,)\n",
    "        log_prob_components = log_mix[None, :] + log_norm[None, :] - 0.5 * mahal  # (N, K)\n",
    "\n",
    "        # Log-sum-exp over components\n",
    "        log_prob = torch.logsumexp(log_prob_components, dim=1)\n",
    "        nll -= torch.mean(log_prob) / num_batches\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrJP7qVWmbo5"
   },
   "source": [
    "### GMM Training Function\n",
    "\n",
    "We parameterize the covariance as $\\Sigma = AA^T + \\epsilon I$ to ensure positive semi-definiteness.\n",
    "\n",
    "The training returns:\n",
    "- `mix`: (K,) mixture weights\n",
    "- `mu`: (K, D) means\n",
    "- `cov`: (K, D, D) covariance matrices\n",
    "- `L`: (K, D, D) Cholesky factors (where cov â‰ˆ L @ L^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbQxLRaFmbo5"
   },
   "outputs": [],
   "source": [
    "def train_gmm_full_cov(patches_train, K, num_epochs=10, batch_size=512, lr=1e-4, device='cpu', verbose=True, eps=1e-4):\n",
    "    \"\"\"\n",
    "    Train GMM with FULL covariance using A*A^T + ÎµI parameterization.\n",
    "\n",
    "    Args:\n",
    "        patches_train: Training data (N, D)\n",
    "        K: Number of mixture components\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        device: 'cpu' or 'cuda'\n",
    "        verbose: Print progress\n",
    "        eps: Regularization constant\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with trained parameters and training history\n",
    "    \"\"\"\n",
    "    N, D = patches_train.shape\n",
    "    EPS = eps\n",
    "\n",
    "    # Reset seeds for reproducibility\n",
    "    np.random.seed(42 + K)  # Different seed per K for variety, but reproducible\n",
    "    torch.manual_seed(42 + K)\n",
    "\n",
    "    # =========================================================================\n",
    "    # TODO: Initialize parameters\n",
    "    # =========================================================================\n",
    "    # 1. Initialize means mu (K, D) by randomly sampling K data points from training set\n",
    "    #    HINT: Use np.random.choice() to select indices without replacement\n",
    "    #\n",
    "    # 2. Initialize A matrices (K, D, D) for covariance Î£ = AA^T + ÎµI\n",
    "    #    HINT: Start with scaled identity matrices. Scale based on data variance.\n",
    "    #\n",
    "    # 3. Initialize mixture logits to zeros (for uniform initial weights)\n",
    "    #    HINT: softmax(zeros) = uniform distribution\n",
    "    # =========================================================================\n",
    "\n",
    "    raise NotImplementedError(\"TODO: Initialize mu, A, and mix_logits\")\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam([mu, mix_logits, A], lr=lr)\n",
    "\n",
    "    # Convert data to tensor once\n",
    "    data_tensor = torch.tensor(patches_train, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Pre-compute identity matrix for regularization\n",
    "    eye_D = torch.eye(D, device=device)\n",
    "\n",
    "    history = {'train_nll': []}\n",
    "\n",
    "    iterator = tqdm(range(num_epochs), desc=f'Training K={K} Full Cov') if verbose else range(num_epochs)\n",
    "    for epoch in iterator:\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            idx = perm[start:end]\n",
    "            x_batch = data_tensor[idx]  # (batch_N, D)\n",
    "            batch_N = x_batch.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # =========================================================================\n",
    "            # TODO: Compute forward pass\n",
    "            # =========================================================================\n",
    "            # 1. Compute mixture weights from logits using softmax\n",
    "            #\n",
    "            # 2. Compute FULL covariance matrices: Î£ = AA^T + ÎµI\n",
    "            #    HINT: Use torch.bmm() for batch matrix multiplication\n",
    "            #\n",
    "            # 3. Compute NLL using Cholesky decomposition:\n",
    "            #    - Compute Cholesky factor L of covariance\n",
    "            #    - log|Î£| = 2 * sum(log(diag(L)))\n",
    "            #    - Compute Mahalanobis distance via triangular solve\n",
    "            #      HINT: Use torch.linalg.solve_triangular() to solve L @ z = diff\n",
    "            #    - Combine into log probability and use logsumexp\n",
    "            #    - loss = -mean(log_prob)\n",
    "            # =========================================================================\n",
    "\n",
    "            raise NotImplementedError(\"TODO: Compute forward pass and loss\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        history['train_nll'].append(avg_loss)\n",
    "\n",
    "        if verbose:\n",
    "            iterator.set_postfix({'NLL': f'{avg_loss:.2f}'})\n",
    "\n",
    "    # Build final covariance matrices and Cholesky factors\n",
    "    with torch.no_grad():\n",
    "        mix_final = torch.softmax(mix_logits, dim=0)\n",
    "        cov_final = torch.bmm(A, A.transpose(1, 2)) + EPS * eye_D.unsqueeze(0)\n",
    "        L_final = torch.linalg.cholesky(cov_final)\n",
    "\n",
    "    return {\n",
    "        'mix': mix_final.cpu(),\n",
    "        'mu': mu.detach().cpu(),\n",
    "        'cov': cov_final.cpu(),\n",
    "        'L': L_final.cpu(),\n",
    "        'A': A.detach().cpu(),\n",
    "        'history': history,\n",
    "        'eps': EPS\n",
    "    }\n",
    "\n",
    "\n",
    "def unpack_cholesky(L):\n",
    "    \"\"\"\n",
    "    Unpack covariance matrix from its Cholesky factor.\n",
    "    Given L (lower triangular), returns cov = L @ L^T\n",
    "    \"\"\"\n",
    "    if L.dim() == 2:\n",
    "        return L @ L.T\n",
    "    else:\n",
    "        return torch.bmm(L, L.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8UXts7Zmbo5"
   },
   "source": [
    "### Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYR_4QIkmbo6"
   },
   "outputs": [],
   "source": [
    "def sample_from_gmm(mix, mu, cov, n_samples):\n",
    "    \"\"\"Sample from a GMM.\"\"\"\n",
    "    K = len(mix)\n",
    "    mix_np = mix.detach().numpy()\n",
    "    mu_np = mu.detach().numpy()\n",
    "    cov_np = cov.detach().numpy()\n",
    "\n",
    "    # Sample component indices\n",
    "    component_idx = np.random.choice(K, size=n_samples, p=mix_np)\n",
    "\n",
    "    # Sample from each component\n",
    "    samples = np.zeros((n_samples, mu_np.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        k = component_idx[i]\n",
    "        samples[i] = np.random.multivariate_normal(mu_np[k], cov_np[k])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Sb340Kmbo6"
   },
   "source": [
    "---\n",
    "## Problem 1.1(a): Closed-form Solution for Single Gaussian\n",
    "\n",
    "**Task:** Estimate $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ using the closed-form MLE solutions:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\mu}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i$$\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82EhKiwxmbo6"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(a): Closed-form solution\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Compute closed-form mean\n",
    "# HINT: Use .mean() with appropriate axis\n",
    "# =========================================================================\n",
    "mu_closed = None  # TODO: Replace this line with your implementation\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Compute closed-form covariance\n",
    "# HINT: Use np.cov() - check the documentation for the correct parameters\n",
    "#       to get MLE estimate (use the variance that divides by N)\n",
    "# =========================================================================\n",
    "cov_closed = None  # TODO: Replace this line with your implementation\n",
    "\n",
    "# --- Check that TODOs are completed ---\n",
    "assert mu_closed is not None, \"TODO: Implement closed-form mean above\"\n",
    "assert cov_closed is not None, \"TODO: Implement closed-form covariance above\"\n",
    "\n",
    "# Add regularization for numerical stability\n",
    "EPS_REG = 1e-4\n",
    "cov_closed = cov_closed + EPS_REG * np.eye(PATCH_SIZE)\n",
    "\n",
    "print(f\"Mean shape: {mu_closed.shape}\")\n",
    "print(f\"Covariance shape: {cov_closed.shape}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "mu_closed_tensor = torch.tensor(mu_closed, dtype=torch.float32).unsqueeze(0)\n",
    "cov_closed_tensor = torch.tensor(cov_closed, dtype=torch.float32).unsqueeze(0)\n",
    "mix_single = torch.ones(1)\n",
    "\n",
    "nll_train_closed = gmm_nll(mix_single, mu_closed_tensor, cov_closed_tensor, patches_train_tensor)\n",
    "nll_test_closed = gmm_nll(mix_single, mu_closed_tensor, cov_closed_tensor, patches_test_tensor)\n",
    "\n",
    "print(f\"\\nClosed-form Single Gaussian:\")\n",
    "print(f\"  Train NLL: {nll_train_closed.item():.4f}\")\n",
    "print(f\"  Test NLL:  {nll_test_closed.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qZloL7Dmbo6"
   },
   "source": [
    "---\n",
    "## Problem 1.1(b): Visualize Top 10 Eigenvectors\n",
    "\n",
    "**Task:** To help understand what the model is capturing, compute the eigendecomposition of the covariance matrix and visualize the top 10 eigenvectors.\n",
    "\n",
    "The eigenvectors represent the principal directions of variation in the data. For natural image patches, these should resemble Fourier/DCT basis functions.\n",
    "\n",
    "**Background on DCT/Fourier bases:**\n",
    "- The **Discrete Cosine Transform (DCT)** and **Fourier Transform** decompose signals into sinusoidal components at different frequencies\n",
    "- DCT/Fourier basis functions look like 2D sinusoidal waves with varying frequencies and orientations\n",
    "- Low-frequency bases are smooth and slowly varying; high-frequency bases have rapid oscillations\n",
    "- Natural images have statistics that are approximately **translation-invariant** (shifting a patch doesn't fundamentally change its statistics)\n",
    "- For translation-invariant processes, the eigenvectors of the covariance matrix are exactly the Fourier/DCT basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoimkPUEmbo6"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(b): Visualize top 10 eigenvectors\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Compute eigendecomposition\n",
    "# HINT: Use np.linalg.eigh() for symmetric matrices\n",
    "# NOTE: np.linalg.eigh() returns eigenvalues in ASCENDING order!\n",
    "# =========================================================================\n",
    "eigenvalues, eigenvectors = None, None  # TODO: Replace with your implementation\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Sort eigenvalues and eigenvectors in DESCENDING order\n",
    "# HINT: Use np.argsort() with [::-1] to reverse, then index both arrays\n",
    "# =========================================================================\n",
    "# TODO: Add sorting code here\n",
    "\n",
    "# --- Check that TODOs are completed ---\n",
    "assert eigenvalues is not None, \"TODO: Implement eigendecomposition above\"\n",
    "\n",
    "# Plot top 10 eigenvectors\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eigvec = eigenvectors[:, i].reshape(PATCH_DIM, PATCH_DIM)\n",
    "    ax.imshow(eigvec, cmap='gray')\n",
    "    ax.set_title(f'$\\lambda_{{{i+1}}}$ = {eigenvalues[i]:.3f}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Top 10 Eigenvectors of Covariance Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot eigenvalue spectrum\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.semilogy(eigenvalues, 'b-')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Eigenvalue (log scale)')\n",
    "plt.title('Eigenvalue Spectrum of Covariance Matrix')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHfMVpqambo7"
   },
   "source": [
    "### TODO: Describe the eigenvectors (Problem 1.1(b))\n",
    "\n",
    "**Question:** Describe the appearance of the eigenvectors in words. Consider:\n",
    "- Do they look like smooth waves? Sharp edges? Random noise?\n",
    "- How do they relate to the DCT/Fourier bases described above?\n",
    "\n",
    "**Your Answer:** *(Double-click to edit this cell and write your answer here)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmApRHjsmbo7"
   },
   "source": [
    "---\n",
    "## Problem 1.1(c): Covariance Structure Analysis\n",
    "\n",
    "**Task:** Visualize the covariance between a fixed pixel and all other pixels.\n",
    "\n",
    "This helps explain the connection between the eigenvalues and the Fourier/DCT bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY8CuE-Fmbo7"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(c): Covariance structure visualization\n",
    "\n",
    "# Pick a few reference pixels\n",
    "ref_pixels = [\n",
    "    (8, 8),   # Center\n",
    "    (4, 4),   # Top-left quadrant\n",
    "    (4, 12),  # Top-right quadrant\n",
    "    (12, 8),  # Bottom center\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(ref_pixels), figsize=(12, 3))\n",
    "for i, (py, px) in enumerate(ref_pixels):\n",
    "    # Convert 2D pixel location to 1D index\n",
    "    p_idx = py * PATCH_DIM + px\n",
    "\n",
    "    # Get covariance of this pixel with all others\n",
    "    cov_row = cov_closed[p_idx, :].reshape(PATCH_DIM, PATCH_DIM)\n",
    "\n",
    "    axes[i].imshow(cov_row, cmap='RdBu_r')\n",
    "    axes[i].scatter([px], [py], c='green', s=100, marker='x', linewidths=3)\n",
    "    axes[i].set_title(f'Cov with pixel ({py},{px})')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Covariance Structure: Cov[P_{p*}, P_{q}] for different reference pixels p*', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0evNPzpdmbo7"
   },
   "source": [
    "### TODO: Describe the covariance structure (Problem 1.1(c))\n",
    "\n",
    "**Question:** Describe what you observe about the covariance structure. Consider:\n",
    "- Is it similar across different reference pixels?\n",
    "- What does this suggest about the statistics of natural images?\n",
    "\n",
    "**Your Answer:** *(Double-click to edit this cell and write your answer here)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74GrD2xAmbo7"
   },
   "source": [
    "---\n",
    "## Problem 1.1(d): SGD for Single Gaussian (K=1)\n",
    "\n",
    "**Task:** Train a single Gaussian using stochastic gradient descent and compare to the closed-form solution.\n",
    "\n",
    "Use:\n",
    "- Batch size: 4096\n",
    "- Learning rate: 1e-4\n",
    "- Epochs: 20\n",
    "- Regularization: Îµ = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07nj-8QRmbo7"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(d): Train single Gaussian with SGD\n",
    "print(\"Training single Gaussian (K=1) with SGD...\")\n",
    "print(\"Hyperparameters: batch_size=4096, lr=1e-4, num_epochs=20, eps=1e-4\")\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Train K=1 GMM using train_gmm_full_cov()\n",
    "#\n",
    "# Call train_gmm_full_cov() with the following arguments:\n",
    "#   - patches_train: the training data\n",
    "#   - K=1: number of components\n",
    "#   - num_epochs=20: number of training epochs\n",
    "#   - batch_size=4096: batch size\n",
    "#   - lr=1e-4: learning rate\n",
    "#   - device=device: 'cpu' or 'cuda'\n",
    "#   - eps=1e-4: regularization constant\n",
    "# =========================================================================\n",
    "result_k1 = None  # TODO: Replace with your function call\n",
    "\n",
    "# --- Check that TODO is completed ---\n",
    "assert result_k1 is not None, \"TODO: Call train_gmm_full_cov() above\"\n",
    "\n",
    "# Evaluate on test set\n",
    "test_nll_sgd_k1 = gmm_nll(result_k1['mix'], result_k1['mu'], result_k1['cov'], patches_test_tensor)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Closed-form Test NLL: {nll_test_closed.item():.4f}\")\n",
    "print(f\"  SGD Test NLL:         {test_nll_sgd_k1.item():.4f}\")\n",
    "print(f\"  Difference:           {abs(nll_test_closed.item() - test_nll_sgd_k1.item()):.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(result_k1['history']['train_nll'], 'b-o', label='SGD')\n",
    "plt.axhline(y=nll_train_closed.item(), color='r', linestyle='--', label='Closed-form')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.title('K=1 SGD Training vs Closed-form Solution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98MmWSfImbo7"
   },
   "source": [
    "---\n",
    "## Problem 1.1(e): GMM with K=16\n",
    "\n",
    "**Task:** Train a GMM with K=16 components and compare to K=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjakOhRdmbo8"
   },
   "source": [
    "---\n",
    "### Warm-up: 2-Component GMM (K=2)\n",
    "\n",
    "Before training with K=16, let's first visualize a simple 2-component GMM to build intuition about how mixture components specialize.\n",
    "\n",
    "**Task:** Train a 2-component GMM and compare the eigenvector structure of both components:\n",
    "- Display top 63 eigenvectors (9Ã—7 grid) for each component\n",
    "- Observe that both components have approximately the same eigenvectors (Fourier-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDhgNlZKmbo8"
   },
   "outputs": [],
   "source": [
    "def visualize_figure4_style(result_k2, patch_dim=16):\n",
    "    \"\"\"\n",
    "    Recreate Figure 4 from Zoran & Weiss paper.\n",
    "    Shows eigenvectors (9x7 dense grid) for a 2-component GMM.\n",
    "    \"\"\"\n",
    "    K = 2\n",
    "    n_eigvec_cols, n_eigvec_rows = 9, 7  # 63 eigenvectors per component\n",
    "    n_eigvecs = n_eigvec_cols * n_eigvec_rows\n",
    "    border_width = 1\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "    colors = ['blue', 'red']\n",
    "\n",
    "    for k in range(K):\n",
    "        cov_k = result_k2['cov'][k].numpy()\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_k)\n",
    "\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        grid_h = n_eigvec_rows * patch_dim + (n_eigvec_rows + 1) * border_width\n",
    "        grid_w = n_eigvec_cols * patch_dim + (n_eigvec_cols + 1) * border_width\n",
    "        eigvec_composite = np.ones((grid_h, grid_w)) * 0.5\n",
    "\n",
    "        for i in range(n_eigvecs):\n",
    "            row = i // n_eigvec_cols\n",
    "            col = i % n_eigvec_cols\n",
    "            eigvec = eigenvectors[:, i].reshape(patch_dim, patch_dim)\n",
    "            eigvec_norm = eigvec / (np.abs(eigvec).max() + 1e-8)\n",
    "            eigvec_display = (eigvec_norm + 1) / 2\n",
    "\n",
    "            y_start = border_width + row * (patch_dim + border_width)\n",
    "            x_start = border_width + col * (patch_dim + border_width)\n",
    "            eigvec_composite[y_start:y_start+patch_dim, x_start:x_start+patch_dim] = eigvec_display\n",
    "\n",
    "        axes[k].imshow(eigvec_composite, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[k].set_title(f'Component {k+1}: Ï€ = {result_k2[\"mix\"][k].item():.4f}',\n",
    "                         fontsize=12, color=colors[k])\n",
    "        axes[k].axis('off')\n",
    "\n",
    "    plt.suptitle('Figure 4 Style: 2-Component GMM Eigenvectors\\n'\n",
    "                 '(9Ã—7 grid of eigenvectors sorted by decreasing eigenvalue)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SknhShegmbo8"
   },
   "outputs": [],
   "source": [
    "# Train K=2 GMM for eigenvector analysis\n",
    "print(\"Training K=2 GMM for eigenvector analysis...\")\n",
    "\n",
    "# Train K=2 GMM\n",
    "result_k2 = train_gmm_full_cov(patches_train, K=2, num_epochs=15, batch_size=4096,\n",
    "                               lr=1e-4, device=device, eps=1e-4)\n",
    "\n",
    "print(f\"\\nK=2 GMM trained:\")\n",
    "print(f\"  Component 1 weight: {result_k2['mix'][0].item():.4f}\")\n",
    "print(f\"  Component 2 weight: {result_k2['mix'][1].item():.4f}\")\n",
    "\n",
    "# Create Figure 4 style visualization\n",
    "visualize_figure4_style(result_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AsdhICImbo8"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(e): Train GMM with K=16\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Train K=16 GMM using train_gmm_full_cov()\n",
    "#\n",
    "# Call train_gmm_full_cov() with the following arguments:\n",
    "#   - patches_train: the training data\n",
    "#   - K=16: number of components\n",
    "#   - num_epochs=64: number of training epochs\n",
    "#   - batch_size=4096: batch size\n",
    "#   - lr=1e-4: learning rate\n",
    "#   - device=device: 'cpu' or 'cuda'\n",
    "#   - eps=1e-4: regularization constant\n",
    "# =========================================================================\n",
    "result_k16 = None  # TODO: Replace with your function call\n",
    "\n",
    "# --- Check that TODO is completed ---\n",
    "assert result_k16 is not None, \"TODO: Train K=16 GMM above\"\n",
    "\n",
    "# Evaluate on test set\n",
    "test_nll_k1 = gmm_nll(result_k1['mix'], result_k1['mu'], result_k1['cov'], patches_test_tensor)\n",
    "test_nll_k16 = gmm_nll(result_k16['mix'], result_k16['mu'], result_k16['cov'], patches_test_tensor)\n",
    "\n",
    "print(f\"\\nTest NLL Comparison:\")\n",
    "print(f\"  K=1:  {test_nll_k1.item():.4f}\")\n",
    "print(f\"  K=16: {test_nll_k16.item():.4f}\")\n",
    "print(f\"  Improvement: {test_nll_k1.item() - test_nll_k16.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evRbr4f3mbo8"
   },
   "source": [
    "---\n",
    "## Problem 1.1(f): Compare GMM vs Single Gaussian\n",
    "\n",
    "**Task:** Explain why the GMM has lower NLL than the single Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDyZ4CZLmbo8"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(f): Comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Comparison: Single Gaussian vs GMM (K=16)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Single Gaussian (K=1) Test NLL: {test_nll_k1.item():.4f}\")\n",
    "print(f\"GMM K=16 Test NLL:              {test_nll_k16.item():.4f}\")\n",
    "print(f\"\\nImprovement (K=16 vs K=1): {test_nll_k1.item() - test_nll_k16.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK4bD7Wkmbo9"
   },
   "source": [
    "### TODO: Explain the NLL improvement (Problem 1.1(f))\n",
    "\n",
    "**Question:** Explain why GMM with K=16 has lower NLL than the single Gaussian.\n",
    "\n",
    "**Your Answer:** *(Double-click to edit this cell and write your answer here)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrRWBBfzmbo9"
   },
   "source": [
    "---\n",
    "## Problem 1.1(g): K Sweep\n",
    "\n",
    "**Task:** Train models with K = 1, 4, 16, 32, 64 components. Plot NLL and visualize samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrJnyZ8kmbo9"
   },
   "outputs": [],
   "source": [
    "# Problem 1.1(g): Train models with different K values\n",
    "\n",
    "K_values = [1, 4, 16, 32, 64]\n",
    "results_full = {}\n",
    "\n",
    "# =========================================================================\n",
    "# TODO: Train GMMs for each K value using train_gmm_full_cov()\n",
    "#\n",
    "# For each K in K_values, call train_gmm_full_cov() with:\n",
    "#   - patches_train: the training data\n",
    "#   - K=K: current number of components\n",
    "#   - num_epochs=64: fixed 64 epochs for all K\n",
    "#   - batch_size=4096: batch size\n",
    "#   - lr=1e-4: learning rate\n",
    "#   - device=device: 'cpu' or 'cuda'\n",
    "#   - eps=1e-4: regularization constant\n",
    "#\n",
    "# Store results in results_full dictionary: results_full[K] = result\n",
    "# Also compute and store test NLL: results_full[K]['test_nll'] = test_nll.item()\n",
    "# =========================================================================\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\nTraining K={K} with 64 epochs...\")\n",
    "\n",
    "    # TODO: Call train_gmm_full_cov() here and store in results_full[K]\n",
    "    results_full[K] = None  # TODO: Replace with your function call\n",
    "\n",
    "    # --- Check that TODO is completed ---\n",
    "    assert results_full[K] is not None, f\"TODO: Train K={K} GMM\"\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_nll = gmm_nll(results_full[K]['mix'], results_full[K]['mu'], results_full[K]['cov'], patches_test_tensor)\n",
    "    results_full[K]['test_nll'] = test_nll.item()\n",
    "    print(f\"K={K} Test NLL: {test_nll.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Save the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(results_full, 'gmm_full_cov_results.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = torch.load('gmm_full_cov_results.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMMky1h9mbo-"
   },
   "outputs": [],
   "source": [
    "# Plot NLL vs K\n",
    "test_nlls = [results_full[K]['test_nll'] for K in K_values]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_values, test_nlls, 'b-o', markersize=8, linewidth=2)\n",
    "plt.xlabel('Number of Components (K)')\n",
    "plt.ylabel('Test NLL')\n",
    "plt.title('Test NLL vs Number of Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(K_values, test_nlls, 'b-o', markersize=8, linewidth=2, base=2)\n",
    "plt.xlabel('Number of Components (K, log scale)')\n",
    "plt.ylabel('Test NLL')\n",
    "plt.title('Test NLL vs K (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary: Test NLL vs K\")\n",
    "print(\"-\" * 40)\n",
    "for K in K_values:\n",
    "    print(f\"K = {K:3d}: NLL = {results_full[K]['test_nll']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJo_cL5Rmbo-"
   },
   "outputs": [],
   "source": [
    "# Visualize samples from each model\n",
    "K_values_viz = [1, 4, 16, 64]\n",
    "fig, axes = plt.subplots(len(K_values_viz), 8, figsize=(16, 2*len(K_values_viz)))\n",
    "\n",
    "for row, K in enumerate(K_values_viz):\n",
    "    samples = sample_from_gmm(results_full[K]['mix'], results_full[K]['mu'], results_full[K]['cov'], 8)\n",
    "    samples = samples.reshape(-1, PATCH_DIM, PATCH_DIM)\n",
    "\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(samples[col] + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].set_xticks([])\n",
    "        axes[row, col].set_yticks([])\n",
    "        for spine in axes[row, col].spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    axes[row, 0].set_ylabel(f'K={K}', fontsize=12, rotation=0, labelpad=30, va='center')\n",
    "\n",
    "plt.suptitle('Problem 1.1(g): Samples from GMMs with Different K', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy8mKrwcmbo-"
   },
   "source": [
    "### TODO: Describe performance vs K (Problem 1.1(g))\n",
    "\n",
    "**Question:** Describe how performance changes as the number of Gaussians in the mixture (i.e., K) increases, both:\n",
    "- **Quantitatively:** How does test NLL change as K increases?\n",
    "- **Qualitatively:** How do the samples look different for small vs large K?\n",
    "\n",
    "**Your Answer:** *(Double-click to edit this cell and write your answer here)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fglSnU0mbo-"
   },
   "source": [
    "---\n",
    "## Problem 1.1(h): Analyzing each component of the GMM\n",
    "\n",
    "**Tasks:**\n",
    "1. Analyze mixture weights for K=64\n",
    "2. Identify texture and edge components\n",
    "3. Visualize leading eigenvectors and samples\n",
    "4. Show component assignments on natural images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTsr7eh-mbo_"
   },
   "outputs": [],
   "source": [
    "# Use K=64 model from results\n",
    "result_k64 = results_full[64]\n",
    "\n",
    "# Analyze mixture weights\n",
    "mix_weights = result_k64['mix'].numpy()\n",
    "\n",
    "print(\"\\nMixture Weight Statistics (K=64):\")\n",
    "print(f\"  Max weight: {mix_weights.max():.4f}\")\n",
    "print(f\"  Min weight: {mix_weights.min():.4f}\")\n",
    "print(f\"  Mean weight: {mix_weights.mean():.4f} (expected: {1/64:.4f})\")\n",
    "print(f\"  Components with weight > 1%: {(mix_weights > 0.01).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi6PFuKjmbo_"
   },
   "outputs": [],
   "source": [
    "def assign_patches_to_components(patches, mix, mu, cov):\n",
    "    \"\"\"\n",
    "    Assign each patch to its most likely component.\n",
    "\n",
    "    Returns:\n",
    "        assignments: Array of component indices (N,)\n",
    "        posteriors: Posterior probabilities P(k|x) for each patch (N, K)\n",
    "    \"\"\"\n",
    "    N = len(patches)\n",
    "    K = len(mix)\n",
    "    D = patches.shape[1]\n",
    "\n",
    "    # Convert to numpy if needed\n",
    "    patches_np = patches if isinstance(patches, np.ndarray) else patches.numpy()\n",
    "    mix_np = mix.numpy() if hasattr(mix, 'numpy') else mix\n",
    "    mu_np = mu.numpy() if hasattr(mu, 'numpy') else mu\n",
    "    cov_np = cov.numpy() if hasattr(cov, 'numpy') else cov\n",
    "\n",
    "    # Initialize log probabilities array\n",
    "    log_probs = np.zeros((N, K))\n",
    "\n",
    "    # Compute log p(x, k) = log p(x|k) + log p(k) for each component\n",
    "    for k in range(K):\n",
    "        # =========================================================================\n",
    "        # TODO: Compute log probability for component k\n",
    "        # For each patch x, compute: log p(x|k) + log Ï€_k\n",
    "        #\n",
    "        # The Gaussian log probability is:\n",
    "        # log N(x|Î¼_k, Î£_k) = -0.5 * [D*log(2Ï€) + log|Î£_k| + (x-Î¼_k)^T Î£_k^{-1} (x-Î¼_k)]\n",
    "        #\n",
    "        # HINTS:\n",
    "        # - Use np.linalg.slogdet() to compute log determinant\n",
    "        # - Use np.linalg.solve() to compute Î£_k^{-1} @ diff instead of inverting\n",
    "        # - diff = patches_np - mu_np[k]  # (N, D)\n",
    "        # - mahal = np.sum(diff * np.linalg.solve(cov_np[k] + 1e-6 * np.eye(D), diff.T).T, axis=1)\n",
    "        # =========================================================================\n",
    "\n",
    "        raise NotImplementedError(\"TODO: Compute log_probs[:, k]\")\n",
    "\n",
    "    # Convert to posteriors using softmax (provided - no TODO needed)\n",
    "    log_probs_shifted = log_probs - np.max(log_probs, axis=1, keepdims=True)\n",
    "    posteriors = np.exp(log_probs_shifted)\n",
    "    posteriors = posteriors / np.sum(posteriors, axis=1, keepdims=True)\n",
    "\n",
    "    # Get assignments (provided - no TODO needed)\n",
    "    assignments = np.argmax(posteriors, axis=1)\n",
    "\n",
    "    return assignments, posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJJPrIulmbpA"
   },
   "outputs": [],
   "source": [
    "def visualize_figure5_style(result, selected_components, patch_dim=16):\n",
    "    \"\"\"\n",
    "    Create Figure 5 style visualization.\n",
    "    For each selected component, show:\n",
    "    - Top 64 eigenvectors (8Ã—8 grid)\n",
    "    - 64 samples (8Ã—8 grid)\n",
    "    \"\"\"\n",
    "    n_components = len(selected_components)\n",
    "    if n_components == 0:\n",
    "        print(\"No components to visualize!\")\n",
    "        return\n",
    "\n",
    "    n_grid = 8\n",
    "    n_items = n_grid * n_grid\n",
    "\n",
    "    colors = ['#FF6B6B', '#77DD77', '#6B9FFF', '#FDFD96', '#DDA0DD', '#FFB347', '#87CEEB', '#98D8C8', '#F7DC6F', '#BB8FCE'][:n_components]\n",
    "\n",
    "    fig, axes = plt.subplots(2, n_components, figsize=(4*n_components, 10))\n",
    "\n",
    "    if n_components == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for col, k in enumerate(selected_components):\n",
    "        cov_k = result['cov'][k].numpy()\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_k)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Debug: Print eigenvalue range for first component\n",
    "        if col == 0:\n",
    "            print(f\"Component {k}: eigenvalue range [{eigenvalues.min():.2e}, {eigenvalues.max():.2e}]\")\n",
    "            print(f\"  Top 5 eigenvalues: {eigenvalues[:5]}\")\n",
    "\n",
    "        # Top row: Eigenvectors\n",
    "        eigvec_composite = np.zeros((n_grid * patch_dim, n_grid * patch_dim))\n",
    "        for i in range(n_items):\n",
    "            row = i // n_grid\n",
    "            col_idx = i % n_grid\n",
    "            eigvec = eigenvectors[:, i].reshape(patch_dim, patch_dim)\n",
    "            # Normalize each eigenvector to [-1, 1] for display\n",
    "            max_abs = np.abs(eigvec).max()\n",
    "            if max_abs > 1e-10:\n",
    "                eigvec_norm = eigvec / max_abs\n",
    "            else:\n",
    "                eigvec_norm = eigvec  # Avoid division issues\n",
    "            eigvec_composite[row*patch_dim:(row+1)*patch_dim,\n",
    "                           col_idx*patch_dim:(col_idx+1)*patch_dim] = eigvec_norm\n",
    "\n",
    "        axes[0, col].imshow(eigvec_composite, cmap='gray', vmin=-1, vmax=1)\n",
    "        axes[0, col].axis('off')\n",
    "        axes[0, col].set_title(f'Component k={k}\\nÏ€ = {result[\"mix\"][k].item():.4f}',\n",
    "                               fontsize=11, color=colors[col], fontweight='bold')\n",
    "\n",
    "        # Bottom row: Samples from the Gaussian\n",
    "        mu_k = result['mu'][k].numpy()\n",
    "\n",
    "        # Add small regularization to ensure sampling works\n",
    "        cov_k_reg = cov_k + 1e-6 * np.eye(patch_dim * patch_dim)\n",
    "        samples = np.random.multivariate_normal(mu_k, cov_k_reg, size=n_items)\n",
    "\n",
    "        # Debug: Check sample variance for first component\n",
    "        if col == 0:\n",
    "            sample_std = samples.std()\n",
    "            print(f\"  Sample std dev: {sample_std:.4f}\")\n",
    "\n",
    "        sample_composite = np.zeros((n_grid * patch_dim, n_grid * patch_dim))\n",
    "        for i in range(n_items):\n",
    "            row = i // n_grid\n",
    "            col_idx = i % n_grid\n",
    "            sample = samples[i].reshape(patch_dim, patch_dim)\n",
    "            # Add 0.5 to shift from zero-mean to [0, 1] display range\n",
    "            sample_composite[row*patch_dim:(row+1)*patch_dim,\n",
    "                           col_idx*patch_dim:(col_idx+1)*patch_dim] = sample + 0.5\n",
    "\n",
    "        axes[1, col].imshow(np.clip(sample_composite, 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1, col].axis('off')\n",
    "\n",
    "    axes[0, 0].set_ylabel('Eigenvectors\\n(8Ã—8 grid)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Samples\\n(8Ã—8 grid)', fontsize=12)\n",
    "\n",
    "    plt.suptitle(f'Figure 5 Style: Eigenvectors and Samples from {n_components} GMM Components', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z42R9vNRmbpA"
   },
   "source": [
    "### Automatic Component Selection (Heuristic)\n",
    "\n",
    "**DISCLAIMER:** The `identify_component_types_for_image()` function uses heuristics to automatically identify **texture** and **edge** components. However:\n",
    "- It does **NOT** identify **flat/uniform** components - you must find these yourself!\n",
    "- The heuristic may produce incorrect results depending on the trained model\n",
    "\n",
    "**For this problem, you need to identify 3 components:**\n",
    "1. **Edge** - The heuristic can help, or inspect eigenvectors with localized energy\n",
    "2. **Flat/uniform** - Find this yourself! Look for components with low-variance, nearly constant eigenvectors\n",
    "3. **Other** - Your choice! Describe what you see (could be texture, corners, gradients, etc.)\n",
    "\n",
    "**Recommended approach:**\n",
    "1. Run `visualize_figure5_style(result_k64, top_8_indices)` to inspect top components\n",
    "2. Use the heuristic as a starting point for edges\n",
    "3. Look for flat components (samples look uniform, eigenvectors have low variance)\n",
    "4. Pick an interesting \"other\" component and describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyY36IpGmbpA"
   },
   "outputs": [],
   "source": [
    "def identify_component_types_for_image(test_image, result, patch_size=16, stride=4, n_texture=2, n_edge=2):\n",
    "    \"\"\"\n",
    "    Heuristic function to identify texture and edge components.\n",
    "\n",
    "    WARNING: This is a heuristic and may not always produce correct results.\n",
    "    Students should verify selections manually using visualize_figure5_style().\n",
    "\n",
    "    Returns:\n",
    "        selected_components: List of component indices (texture + edge)\n",
    "        assignments: Patch assignments for the test image\n",
    "        posteriors: Posterior probabilities\n",
    "        patches: Extracted patches from test image\n",
    "    \"\"\"\n",
    "    K = len(result['mix'])\n",
    "    PATCH_DIM = patch_size\n",
    "    H, W = test_image.shape\n",
    "\n",
    "    # Extract patches from test image and assign to components\n",
    "    patches = []\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = test_image[y:y+patch_size, x:x+patch_size]\n",
    "            patch_flat = patch.flatten() - patch.mean()\n",
    "            patches.append(patch_flat)\n",
    "    patches = np.array(patches)\n",
    "\n",
    "    # Assign patches to components\n",
    "    assignments, posteriors = assign_patches_to_components(\n",
    "        patches, result['mix'], result['mu'], result['cov']\n",
    "    )\n",
    "\n",
    "    # Count assignments per component\n",
    "    assignment_counts = np.bincount(assignments, minlength=K)\n",
    "\n",
    "    # Thresholds for minimum representation\n",
    "    texture_min_count = len(patches) * 0.01   # 1% for textures\n",
    "    edge_min_count = len(patches) * 0.005     # 0.5% for edges\n",
    "\n",
    "    texture_candidates = []\n",
    "    edge_candidates = []\n",
    "\n",
    "    for k in range(K):\n",
    "        cov_k = result['cov'][k].numpy()\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_k)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        total_var = eigenvalues.sum()\n",
    "        top_eigvec = eigenvectors[:, 0].reshape(PATCH_DIM, PATCH_DIM)\n",
    "\n",
    "        # Check energy distribution for edge detection\n",
    "        left_energy = np.abs(top_eigvec[:, :PATCH_DIM//2]).sum()\n",
    "        right_energy = np.abs(top_eigvec[:, PATCH_DIM//2:]).sum()\n",
    "        top_energy = np.abs(top_eigvec[:PATCH_DIM//2, :]).sum()\n",
    "        bottom_energy = np.abs(top_eigvec[PATCH_DIM//2:, :]).sum()\n",
    "\n",
    "        total_energy = np.abs(top_eigvec).sum()\n",
    "\n",
    "        lr_ratio = max(left_energy, right_energy) / (total_energy + 1e-8)\n",
    "        tb_ratio = max(top_energy, bottom_energy) / (total_energy + 1e-8)\n",
    "\n",
    "        asymmetry = max(lr_ratio, tb_ratio)\n",
    "        count = assignment_counts[k]\n",
    "\n",
    "        if asymmetry > 0.55 and count >= edge_min_count:\n",
    "            edge_candidates.append((k, asymmetry, total_var, count))\n",
    "        elif count >= texture_min_count:\n",
    "            texture_candidates.append((k, total_var, count, asymmetry))\n",
    "\n",
    "    # Select components\n",
    "    texture_candidates.sort(key=lambda x: (-x[2], -x[1]))\n",
    "    selected_texture = [c[0] for c in texture_candidates[:n_texture]]\n",
    "\n",
    "    edge_candidates.sort(key=lambda x: (-x[1], -x[3]))\n",
    "    selected_edge = [c[0] for c in edge_candidates[:n_edge]]\n",
    "\n",
    "    print(f\"\\n[Heuristic] Component selection:\")\n",
    "    print(f\"  Texture candidates found: {len(texture_candidates)}\")\n",
    "    print(f\"  Edge candidates found: {len(edge_candidates)}\")\n",
    "    print(f\"  Selected texture components: {selected_texture}\")\n",
    "    print(f\"  Selected edge components: {selected_edge}\")\n",
    "    print(f\"\\nâš ï¸  Please verify these selections manually!\")\n",
    "\n",
    "    return selected_texture + selected_edge, assignments, posteriors, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LBj0fsHmbpB"
   },
   "outputs": [],
   "source": [
    "# Example: Visualize the TOP 8 components by mixture weight\n",
    "# These are the most important components (highest Ï€ values)\n",
    "mix_weights = result_k64['mix'].numpy()\n",
    "top_8_indices = np.argsort(mix_weights)[::-1][:8].tolist()  # Sort descending, take top 8\n",
    "\n",
    "print(\"Visualizing TOP 8 components by mixture weight:\")\n",
    "print(f\"Component indices: {top_8_indices}\")\n",
    "print(f\"Mixture weights: {[f'{mix_weights[k]:.4f}' for k in top_8_indices]}\")\n",
    "print(\"(Look for: texture = Fourier-like patterns, edge = localized energy)\")\n",
    "_ = visualize_figure5_style(result_k64, top_8_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6annxCPmbpB"
   },
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# TODO: Identify 3 components from the K=64 model\n",
    "#\n",
    "# Find components that capture each of the following image structures:\n",
    "#   (1) Edges - sharp transitions between light and dark regions\n",
    "#   (2) Flat/uniform regions - patches with little variation\n",
    "#   (3) Other - any other type of content you find interesting (describe it!)\n",
    "#\n",
    "# HINTS:\n",
    "# - Edge components have localized eigenvectors (energy on one side)\n",
    "# - Flat components have low-variance eigenvectors (nearly constant)\n",
    "# - Texture components have Fourier-like eigenvectors (periodic patterns)\n",
    "#\n",
    "# The heuristic function `identify_component_types_for_image()` can help\n",
    "# identify texture and edge components, but you must find the FLAT\n",
    "# component yourself by inspecting the visualizations above.\n",
    "#\n",
    "# After inspecting components, enter your chosen indices below.\n",
    "# =========================================================================\n",
    "selected_components = None  # TODO: e.g., [5, 12, 23] for edge, flat, other\n",
    "\n",
    "# --- Check that TODO is completed ---\n",
    "assert selected_components is not None, \"TODO: Select 3 components (edge, flat, other) above\"\n",
    "assert len(selected_components) == 3, \"TODO: You should select exactly 3 components\"\n",
    "\n",
    "# Visualize selected components\n",
    "component_colors = visualize_figure5_style(result_k64, selected_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFsHPcZqmbpB"
   },
   "source": [
    "### Figure 6: Component Assignments on Natural Images\n",
    "\n",
    "This visualization shows which GMM component is most likely for each patch location in a natural image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pEE4fabmbpB"
   },
   "outputs": [],
   "source": [
    "def visualize_figure6_style(test_image, result, selected_components, colors,\n",
    "                            assignments, posteriors, patch_size=16, stride=4,\n",
    "                            max_patches_per_component=40):\n",
    "    \"\"\"\n",
    "    Recreate Figure 6: Component assignments on natural images.\n",
    "    Shows spatially distributed patches for each component (non-overlapping within same component).\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    H, W = test_image.shape\n",
    "\n",
    "    # Get patch positions\n",
    "    positions = []\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            positions.append((y, x))\n",
    "\n",
    "    positions = np.array(positions)\n",
    "    print(f\"Total patches: {len(positions)}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    ax.imshow(test_image, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    # For each selected component, select spatially distributed patches\n",
    "    stats = {}\n",
    "    for color_idx, k in enumerate(selected_components):\n",
    "        # Find patches assigned to this component\n",
    "        component_mask = assignments == k\n",
    "        component_indices = np.where(component_mask)[0]\n",
    "\n",
    "        if len(component_indices) == 0:\n",
    "            stats[k] = 0\n",
    "            continue\n",
    "\n",
    "        stats[k] = len(component_indices)\n",
    "\n",
    "        # Get positions and posteriors for this component's patches\n",
    "        component_positions = positions[component_indices]\n",
    "        component_posteriors = posteriors[component_indices, k]\n",
    "\n",
    "        # Greedy selection for spatial distribution (non-overlapping)\n",
    "        n_select = min(max_patches_per_component, len(component_indices))\n",
    "        selected_local = []\n",
    "        selected_positions = []\n",
    "\n",
    "        # Sort by posterior (descending) to prioritize high-confidence patches\n",
    "        sorted_order = np.argsort(component_posteriors)[::-1]\n",
    "\n",
    "        for idx in sorted_order:\n",
    "            if len(selected_local) >= n_select:\n",
    "                break\n",
    "\n",
    "            pos = component_positions[idx]\n",
    "\n",
    "            # Check if this patch overlaps with any already selected patch\n",
    "            overlaps = False\n",
    "            for sel_pos in selected_positions:\n",
    "                # Patches overlap if their centers are within patch_size of each other\n",
    "                if abs(pos[0] - sel_pos[0]) < patch_size and abs(pos[1] - sel_pos[1]) < patch_size:\n",
    "                    overlaps = True\n",
    "                    break\n",
    "\n",
    "            if not overlaps:\n",
    "                selected_local.append(idx)\n",
    "                selected_positions.append(pos)\n",
    "\n",
    "        # Draw rectangles for selected patches\n",
    "        for idx in selected_local:\n",
    "            global_idx = component_indices[idx]\n",
    "            y, x = positions[global_idx]\n",
    "\n",
    "            rect = Rectangle((x, y), patch_size, patch_size,\n",
    "                            linewidth=2, edgecolor=colors[color_idx],\n",
    "                            facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Figure 6 Style: Component Assignments on Natural Image\\n'\n",
    "                 f'(Up to {max_patches_per_component} spatially distributed patches per component)')\n",
    "\n",
    "    # Add legend\n",
    "    legend_elements = [Rectangle((0,0), 1, 1, facecolor='none',\n",
    "                                  edgecolor=colors[i], linewidth=3,\n",
    "                                  label=f'Component {selected_components[i]}')\n",
    "                      for i in range(len(selected_components))]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print assignment statistics\n",
    "    print(\"\\nComponent Assignment Statistics (total assigned / shown):\")\n",
    "    for i, k in enumerate(selected_components):\n",
    "        total = stats.get(k, 0)\n",
    "        pct = 100 * total / len(assignments)\n",
    "        print(f\"  Component {k} ({colors[i]}): {total} total ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D4SBh4pmbpB"
   },
   "outputs": [],
   "source": [
    "# Create Figure 6 visualization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Figure 6: Component Assignments on Natural Image\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use first test image\n",
    "test_img = test_images[0]\n",
    "H, W = test_img.shape\n",
    "patch_size = 16\n",
    "stride = 4\n",
    "\n",
    "# Extract patches from test image\n",
    "patches_for_assignment = []\n",
    "for y in range(0, H - patch_size + 1, stride):\n",
    "    for x in range(0, W - patch_size + 1, stride):\n",
    "        patch = test_img[y:y+patch_size, x:x+patch_size]\n",
    "        patch_flat = patch.flatten() - patch.mean()\n",
    "        patches_for_assignment.append(patch_flat)\n",
    "patches_for_assignment = np.array(patches_for_assignment)\n",
    "\n",
    "print(f\"Extracted {len(patches_for_assignment)} patches from test image\")\n",
    "\n",
    "# Assign patches to components\n",
    "assignments, posteriors = assign_patches_to_components(\n",
    "    patches_for_assignment, result_k64['mix'], result_k64['mu'], result_k64['cov']\n",
    ")\n",
    "\n",
    "# Visualize Figure 6\n",
    "visualize_figure6_style(\n",
    "    test_img, result_k64, selected_components, component_colors,\n",
    "    assignments, posteriors, patch_size=16, stride=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAf83zRtmbpB"
   },
   "outputs": [],
   "source": [
    "# Figure 6 on multiple test images\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Figure 6: Component Assignments on Multiple Natural Images\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_images_to_show = min(4, len(test_images))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "for img_idx in range(n_images_to_show):\n",
    "    test_img = test_images[img_idx]\n",
    "    H, W = test_img.shape\n",
    "\n",
    "    # Extract patches\n",
    "    patches_img = []\n",
    "    positions_img = []\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = test_img[y:y+patch_size, x:x+patch_size]\n",
    "            patch_flat = patch.flatten() - patch.mean()\n",
    "            patches_img.append(patch_flat)\n",
    "            positions_img.append((y, x))\n",
    "    patches_img = np.array(patches_img)\n",
    "    positions_img = np.array(positions_img)\n",
    "\n",
    "    # Assign patches\n",
    "    assignments_img, posteriors_img = assign_patches_to_components(\n",
    "        patches_img, result_k64['mix'], result_k64['mu'], result_k64['cov']\n",
    "    )\n",
    "\n",
    "    # Draw on subplot\n",
    "    ax = axes[img_idx]\n",
    "    ax.imshow(test_img, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "    # Draw patches for each selected component\n",
    "    for color_idx, k in enumerate(selected_components):\n",
    "        component_mask = assignments_img == k\n",
    "        component_indices = np.where(component_mask)[0]\n",
    "\n",
    "        if len(component_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        component_positions = positions_img[component_indices]\n",
    "        component_posteriors = posteriors_img[component_indices, k]\n",
    "\n",
    "        # Greedy selection for non-overlapping patches\n",
    "        n_select = min(30, len(component_indices))\n",
    "        selected_local = []\n",
    "        selected_positions = []\n",
    "\n",
    "        sorted_order = np.argsort(component_posteriors)[::-1]\n",
    "\n",
    "        for idx in sorted_order:\n",
    "            if len(selected_local) >= n_select:\n",
    "                break\n",
    "            pos = component_positions[idx]\n",
    "            overlaps = False\n",
    "            for sel_pos in selected_positions:\n",
    "                if abs(pos[0] - sel_pos[0]) < patch_size and abs(pos[1] - sel_pos[1]) < patch_size:\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            if not overlaps:\n",
    "                selected_local.append(idx)\n",
    "                selected_positions.append(pos)\n",
    "\n",
    "        for idx in selected_local:\n",
    "            global_idx = component_indices[idx]\n",
    "            y, x = positions_img[global_idx]\n",
    "            rect = Rectangle((x, y), patch_size, patch_size,\n",
    "                            linewidth=2, edgecolor=component_colors[color_idx], facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    ax.set_title(f'Test Image {img_idx + 1}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [Rectangle((0,0), 1, 1, facecolor='none', edgecolor=component_colors[i],\n",
    "                              linewidth=3, label=f'Component {selected_components[i]}')\n",
    "                  for i in range(len(selected_components))]\n",
    "fig.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.suptitle('Figure 6 Style: Component Assignments on Multiple Natural Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT2eQfirmbpB"
   },
   "source": [
    "---\n",
    "---\n",
    "# Problem 1.2: Image Denoising using GMM\n",
    "\n",
    "We will use the GMM as an image prior for denoising.\n",
    "\n",
    "**Setup:** Given noisy image $\\mathbf{Y} = \\mathbf{X}_0 + \\sigma \\boldsymbol{\\epsilon}$ where $\\sigma = 25/255$.\n",
    "\n",
    "**Goal:** Recover clean image by minimizing:\n",
    "$$\\mathcal{L}(\\hat{\\mathbf{x}}) = \\|\\hat{\\mathbf{x}} - \\mathbf{y}\\|^2 - \\lambda \\log q_\\phi(\\hat{\\mathbf{x}})$$\n",
    "\n",
    "where $\\lambda = 2\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzkCunSjmbpB"
   },
   "source": [
    "## Problem 1.2(a): Patch Denoising with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBq3s1-smbpC"
   },
   "outputs": [],
   "source": [
    "def gmm_log_prob_full(x, mix, mu, cov):\n",
    "    \"\"\"\n",
    "    Compute log probability under GMM with FULL covariance.\n",
    "    Uses Cholesky decomposition for numerical stability.\n",
    "\n",
    "    Args:\n",
    "        x: (N, D) data points\n",
    "        mix: (K,) mixture weights\n",
    "        mu: (K, D) means\n",
    "        cov: (K, D, D) full covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        (N,) log probabilities\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    K = len(mix)\n",
    "    device = x.device\n",
    "\n",
    "    mix = mix.to(device)\n",
    "    mu = mu.to(device)\n",
    "    cov = cov.to(device)\n",
    "\n",
    "    # Cholesky decomposition\n",
    "    cov_reg = cov + 1e-6 * torch.eye(D, device=device).unsqueeze(0)\n",
    "    L = torch.linalg.cholesky(cov_reg)\n",
    "\n",
    "    # Log determinant\n",
    "    log_det = 2 * torch.sum(torch.log(torch.diagonal(L, dim1=1, dim2=2)), dim=1)\n",
    "\n",
    "    # Mahalanobis distance\n",
    "    diff = x.unsqueeze(1) - mu.unsqueeze(0)  # (N, K, D)\n",
    "\n",
    "    mahal = torch.zeros(N, K, device=device)\n",
    "    for k in range(K):\n",
    "        z = torch.linalg.solve_triangular(L[k], diff[:, k, :].T, upper=False)\n",
    "        mahal[:, k] = (z ** 2).sum(dim=0)\n",
    "\n",
    "    # Log probability\n",
    "    log_norm = -0.5 * (D * np.log(2 * np.pi) + log_det)\n",
    "    log_mix = torch.log(mix + 1e-12)\n",
    "    log_prob_components = log_mix + log_norm - 0.5 * mahal\n",
    "\n",
    "    return torch.logsumexp(log_prob_components, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pU1Slp1cmbpC"
   },
   "outputs": [],
   "source": [
    "def sgd_denoise_patches_full_cov(noisy_patches, mix, mu, cov, sigma,\n",
    "                                  lambda_prior=None, num_iters=100, lr=0.1,\n",
    "                                  batch_size=4096, verbose=True):\n",
    "    \"\"\"\n",
    "    SGD-based patch denoising with FULL covariance support.\n",
    "\n",
    "    Minimizes: L(xÌ‚) = ||xÌ‚ - y||Â² - Î» log q(xÌ‚)\n",
    "\n",
    "    Args:\n",
    "        noisy_patches: (N, D) noisy patches (numpy array)\n",
    "        mix, mu, cov: GMM parameters (torch tensors)\n",
    "        sigma: noise standard deviation\n",
    "        lambda_prior: weight for prior term (default: 2ÏƒÂ²)\n",
    "        num_iters: number of gradient descent iterations\n",
    "        lr: learning rate\n",
    "        batch_size: batch size for GPU processing\n",
    "        verbose: show progress bar\n",
    "\n",
    "    Returns:\n",
    "        (N, D) denoised patches (numpy array)\n",
    "    \"\"\"\n",
    "    N, D = noisy_patches.shape\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    if lambda_prior is None:\n",
    "        lambda_prior = 2 * sigma ** 2\n",
    "\n",
    "    mix_gpu = mix.to(device)\n",
    "    mu_gpu = mu.to(device)\n",
    "    cov_gpu = cov.to(device)\n",
    "\n",
    "    all_denoised = []\n",
    "\n",
    "    iterator = tqdm(range(0, N, batch_size), desc='SGD Full Cov') if verbose else range(0, N, batch_size)\n",
    "\n",
    "    for start in iterator:\n",
    "        end = min(start + batch_size, N)\n",
    "        batch = torch.tensor(noisy_patches[start:end], dtype=torch.float32, device=device)\n",
    "\n",
    "        # =========================================================================\n",
    "        # TODO: Implement SGD denoising\n",
    "        # 1. Initialize x_hat with noisy patches: x_hat = batch.clone().requires_grad_(True)\n",
    "        # 2. Create optimizer: optimizer = torch.optim.Adam([x_hat], lr=lr)\n",
    "        # 3. For each iteration:\n",
    "        #    a) optimizer.zero_grad()\n",
    "        #    b) Compute data term: ||x_hat - batch||^2\n",
    "        #    c) Compute prior term: -log q(x_hat) using gmm_log_prob_full\n",
    "        #    d) loss = data_term + lambda_prior * prior_term\n",
    "        #    e) loss.backward() and optimizer.step()\n",
    "        # =========================================================================\n",
    "        raise NotImplementedError(\"TODO: Implement SGD denoising loop\")\n",
    "\n",
    "        all_denoised.append(x_hat.detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_denoised, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP_G4fJHmbpC"
   },
   "outputs": [],
   "source": [
    "# Noise level\n",
    "SIGMA = 25 / 255\n",
    "\n",
    "# Prepare noisy test patches\n",
    "test_subset = patches_test[:500]\n",
    "np.random.seed(42)\n",
    "noisy_patches = test_subset + SIGMA * np.random.randn(*test_subset.shape)\n",
    "\n",
    "print(f\"Test patches: {test_subset.shape}\")\n",
    "print(f\"Noise level: Ïƒ = {SIGMA:.4f} ({SIGMA*255:.0f}/255)\")\n",
    "\n",
    "# Denoise with K=1\n",
    "print(\"\\nDenoising with K=1...\")\n",
    "denoised_k1 = sgd_denoise_patches_full_cov(\n",
    "    noisy_patches,\n",
    "    result_k1['mix'],\n",
    "    result_k1['mu'],\n",
    "    result_k1['cov'],\n",
    "    SIGMA,\n",
    "    num_iters=100,\n",
    "    lr=0.1,\n",
    "    batch_size=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Compute MSE\n",
    "mse_noisy = np.mean((test_subset - noisy_patches) ** 2)\n",
    "mse_k1 = np.mean((test_subset - denoised_k1) ** 2)\n",
    "\n",
    "print(f\"\\nNoisy MSE:  {mse_noisy:.6f}\")\n",
    "print(f\"K=1 MSE:    {mse_k1:.6f}\")\n",
    "print(f\"Reduction:  {(mse_noisy - mse_k1) / mse_noisy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_U2TZvpmbpC"
   },
   "outputs": [],
   "source": [
    "# Noise level\n",
    "SIGMA = 25 / 255\n",
    "\n",
    "# Prepare noisy test patches\n",
    "test_subset = patches_test[:500]\n",
    "np.random.seed(42)\n",
    "noisy_patches = test_subset + SIGMA * np.random.randn(*test_subset.shape)\n",
    "\n",
    "print(f\"Test patches: {test_subset.shape}\")\n",
    "print(f\"Noise level: Ïƒ = {SIGMA:.4f} ({SIGMA*255:.0f}/255)\")\n",
    "\n",
    "# Denoise with K=1\n",
    "print(\"\\nDenoising with K=64...\")\n",
    "denoised_k64 = sgd_denoise_patches_full_cov(\n",
    "    noisy_patches,\n",
    "    result_k64['mix'],\n",
    "    result_k64['mu'],\n",
    "    result_k64['cov'],\n",
    "    SIGMA,\n",
    "    num_iters=100,\n",
    "    lr=0.1,\n",
    "    batch_size=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Compute MSE\n",
    "mse_noisy = np.mean((test_subset - noisy_patches) ** 2)\n",
    "mse_k64 = np.mean((test_subset - denoised_k64) ** 2)\n",
    "\n",
    "print(f\"\\nNoisy MSE:  {mse_noisy:.6f}\")\n",
    "print(f\"K=64 MSE:    {mse_k64:.6f}\")\n",
    "print(f\"Reduction:  {(mse_noisy - mse_k64) / mse_noisy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGlCd3k1mbpC"
   },
   "outputs": [],
   "source": [
    "# Visualize denoising results\n",
    "n_show = 8\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(16, 6))\n",
    "\n",
    "for i in range(n_show):\n",
    "    axes[0, i].imshow(test_subset[i].reshape(PATCH_DIM, PATCH_DIM) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(noisy_patches[i].reshape(PATCH_DIM, PATCH_DIM) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    axes[2, i].imshow(denoised_k1[i].reshape(PATCH_DIM, PATCH_DIM) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Clean', fontsize=12, rotation=0, labelpad=30, va='center')\n",
    "axes[1, 0].set_ylabel('Noisy', fontsize=12, rotation=0, labelpad=30, va='center')\n",
    "axes[2, 0].set_ylabel('Denoised', fontsize=12, rotation=0, labelpad=30, va='center')\n",
    "\n",
    "plt.suptitle('Problem 1.2(a): Patch Denoising with K=1 GMM', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwzpBo-AmbpC"
   },
   "source": [
    "---\n",
    "## Problem 1.2(b): Full Image Denoising\n",
    "\n",
    "**Task:** Apply patch-based denoising to a full image using overlapping patches with stride 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjoZyiXKmbpC"
   },
   "outputs": [],
   "source": [
    "def denoise_image_full_cov(noisy_image, mix, mu, cov, sigma, patch_size=16, stride=4,\n",
    "                           num_iters=200, lr=0.05, batch_size=4096):\n",
    "    \"\"\"\n",
    "    Denoise full image using patch-based SGD with GMM prior.\n",
    "    \"\"\"\n",
    "    H, W = noisy_image.shape\n",
    "    patches = []\n",
    "    positions = []\n",
    "    patch_means = []\n",
    "\n",
    "    # Extract patches with DC removal\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = noisy_image[y:y+patch_size, x:x+patch_size]\n",
    "            patch_mean = patch.mean()\n",
    "            patch_flat = patch.flatten() - patch_mean\n",
    "            patches.append(patch_flat)\n",
    "            positions.append((y, x))\n",
    "            patch_means.append(patch_mean)\n",
    "\n",
    "    patches = np.array(patches, dtype=np.float32)\n",
    "    patch_means = np.array(patch_means, dtype=np.float32)\n",
    "    print(f\"  Extracted {len(patches)} patches (stride={stride})\")\n",
    "\n",
    "    # Denoise patches\n",
    "    denoised_patches = sgd_denoise_patches_full_cov(\n",
    "        patches, mix, mu, cov, sigma,\n",
    "        num_iters=num_iters, lr=lr, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Add back DC component\n",
    "    denoised_patches = denoised_patches + patch_means[:, None]\n",
    "\n",
    "    # Reconstruct by averaging overlapping patches\n",
    "    reconstructed = np.zeros((H, W), dtype=np.float32)\n",
    "    counts = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    for i, (y, x) in enumerate(positions):\n",
    "        patch_2d = denoised_patches[i].reshape(patch_size, patch_size)\n",
    "        reconstructed[y:y+patch_size, x:x+patch_size] += patch_2d\n",
    "        counts[y:y+patch_size, x:x+patch_size] += 1\n",
    "\n",
    "    reconstructed /= np.maximum(counts, 1)\n",
    "    return np.clip(reconstructed, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXrNSdDYmbpC"
   },
   "outputs": [],
   "source": [
    "# Load test image and add noise\n",
    "test_image = test_images[0]\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "noisy_image = test_image + SIGMA * np.random.randn(*test_image.shape)\n",
    "noisy_image = np.clip(noisy_image, 0, 1)\n",
    "\n",
    "psnr_noisy = 10 * np.log10(1.0 / np.mean((test_image - noisy_image) ** 2))\n",
    "print(f\"Noisy PSNR: {psnr_noisy:.2f} dB\")\n",
    "\n",
    "# Denoise\n",
    "print(\"\\nDenoising with K=1...\")\n",
    "denoised_k1_img = denoise_image_full_cov(\n",
    "    noisy_image,\n",
    "    result_k1['mix'], result_k1['mu'], result_k1['cov'],\n",
    "    SIGMA, stride=4, num_iters=200, lr=0.05\n",
    ")\n",
    "psnr_k1 = 10 * np.log10(1.0 / np.mean((test_image - denoised_k1_img) ** 2))\n",
    "print(f\"K=1 PSNR: {psnr_k1:.2f} dB\")\n",
    "\n",
    "print(\"\\nDenoising with K=64...\")\n",
    "denoised_k64_img = denoise_image_full_cov(\n",
    "    noisy_image,\n",
    "    results_full[64]['mix'], results_full[64]['mu'], results_full[64]['cov'],\n",
    "    SIGMA, stride=4, num_iters=200, lr=0.05\n",
    ")\n",
    "psnr_k64 = 10 * np.log10(1.0 / np.mean((test_image - denoised_k64_img) ** 2))\n",
    "print(f\"K=64 PSNR: {psnr_k64:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAv_SXV6mbpD"
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "axes[0, 0].imshow(test_image, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title('Clean')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(noisy_image, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title(f'Noisy\\n(PSNR: {psnr_noisy:.2f} dB)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(denoised_k1_img, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title(f'SGD K=1\\n(PSNR: {psnr_k1:.2f} dB)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(denoised_k64_img, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 3].set_title(f'SGD K=64\\n(PSNR: {psnr_k64:.2f} dB)')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Error maps\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 1].imshow(np.abs(test_image - noisy_image), cmap='hot', vmin=0, vmax=0.3)\n",
    "axes[1, 1].set_title('|Noisy - Clean|')\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 2].imshow(np.abs(test_image - denoised_k1_img), cmap='hot', vmin=0, vmax=0.3)\n",
    "axes[1, 2].set_title('|K=1 - Clean|')\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 3].imshow(np.abs(test_image - denoised_k64_img), cmap='hot', vmin=0, vmax=0.3)\n",
    "axes[1, 3].set_title('|K=64 - Clean|')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Images', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "axes[1, 0].set_ylabel('Errors', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "\n",
    "plt.suptitle('Problem 1.2(b): Full Image Denoising', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save the notebook to PDF for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and download a PDF for this notebook.\n",
    "# Please provide the full path of the notebook file below (we have provided a default filename, but this might not match yours!)\n",
    "#\n",
    "# *If you run into problems running this code, please see this guide*: https://docs.google.com/document/d/1Y5SpnkX4Cp7bPL72CI9B5YSv5tUCiDHxVg3w0L5MXXU/edit?usp=sharing\n",
    "notebook_path = '/content/drive/My Drive/Colab Notebooks/PS1_GMM_Student_v2.ipynb' # UPDATE THIS\n",
    "\n",
    "import os\n",
    "drive_mount_point = '/content/drive/'\n",
    "from google.colab import drive\n",
    "drive.mount(drive_mount_point)\n",
    "file_name = notebook_path.split('/')[-1]\n",
    "get_ipython().system(\"apt update && apt install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\")\n",
    "get_ipython().system(\"pip install pypandoc\")\n",
    "get_ipython().system(\"apt-get install texlive texlive-xetex texlive-latex-extra pandoc\")\n",
    "get_ipython().system(\"jupyter nbconvert --to PDF '{}'\".format(notebook_path))\n",
    "\n",
    "from google.colab import files\n",
    "files.download(notebook_path.split('.')[0]+'.pdf')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
